{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Classification\n",
    "\n",
    "In this notebook, we will demonstrate using a custom SagemMaker PyTorch container to train an acoustic classification model in SageMaker script mode.\n",
    "\n",
    "In this example, the model take reference to the paper VERY DEEP CONVOLUTIONAL NEURAL NETWORKS FOR RAW WAVEFORMS by Wei Dai et al., you can get more information by reading the paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We will use the UrbanSound8K dataset to train our network. It is available for free here <https://urbansounddataset.weebly.com/> and contains 10 audio classes with over 8000 audio samples. Once you have downloaded the compressed dataset, extract it to your current working directory. There is a csv files that contain metadata of all the sound wave metadata.\n",
    "\n",
    "Alternatively, the dataset is also available on Kaggle <https://www.kaggle.com/chrisfilo/urbansound8k/download>.\n",
    "\n",
    "The following are the class labels:\n",
    "```\n",
    "0 = airconditioner \n",
    "1 = carhorn\n",
    "2 = childrenplaying \n",
    "3 = dogbark\n",
    "4 = drilling\n",
    "5 = engineidling \n",
    "6 = gunshot\n",
    "7 = jackhammer\n",
    "8 = siren\n",
    "9 = street_music\n",
    "```\n",
    "\n",
    "\n",
    "The expected directory structure is as follows with respect to this notebook:\n",
    "\n",
    "```\n",
    "../data/UrbanSound8K/\n",
    "|-- fold1\n",
    "|   |-- 1.wav\n",
    "|-- fold2\n",
    "|   |-- 2.wav\n",
    "...\n",
    "|   \n",
    "`-- UrbanSound8K.csv\n",
    "```\n",
    "\n",
    "Let's take a look at a sample file to ensure dataset is downloaded to the correct location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "filename = '../data/UrbanSound8K/fold1/101415-3-0-2.wav'\n",
    "Audio(filename, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create custom container based on SageMaker PyTorch Deep Learning Framework\n",
    "\n",
    "Set `role` to your SageMaker role arn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"arn:aws:iam::342474125894:role/service-role/xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ecr_repository_name = 'pytorch-audio-classification'\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "print('Account: {}'.format(account_id))\n",
    "print('Region: {}'.format(region))\n",
    "print('Role: {}'.format(role))\n",
    "print('S3 Bucket: {}'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dockerfile\n",
    "\n",
    "We will build a custom container on top of existing SageMaker deep learning container by installing additional linux package `libsndfile1` which is requred by python package `librosa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM 763104351884.dkr.ecr.ap-southeast-1.amazonaws.com/pytorch-training:1.5.1-gpu-py3\n",
    "\n",
    "RUN apt-get update \\\n",
    "    && apt-get install -y  --allow-downgrades --allow-change-held-packages --no-install-recommends \\\n",
    "    libsndfile1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training container\n",
    "\n",
    "Next we will create a script that will build and upload the custom container image into ECR. It has to be in the same region where the job is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build_and_push.sh\n",
    "\n",
    "ACCOUNT_ID=$1\n",
    "REGION=$2\n",
    "REPO_NAME=$3\n",
    "DOCKERFILE=$4\n",
    "SERVER=\"${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com\"\n",
    "\n",
    "echo \"ACCOUNT_ID: ${ACCOUNT_ID}\"\n",
    "echo \"REPO_NAME: ${REPO_NAME}\"\n",
    "echo \"REGION: ${REGION}\"\n",
    "echo \"DOCKERFILE: ${DOCKERFILE}\"\n",
    "\n",
    "# Login to retrieve base container\n",
    "aws ecr get-login-password | docker login --username AWS --password-stdin 763104351884.dkr.ecr.${REGION}.amazonaws.com\n",
    "\n",
    "docker build -q -f ${DOCKERFILE} -t ${REPO_NAME} .\n",
    "\n",
    "docker tag ${REPO_NAME} ${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/${REPO_NAME}:latest\n",
    "\n",
    "aws ecr get-login-password | docker login --username AWS --password-stdin ${SERVER}\n",
    "aws ecr describe-repositories --repository-names ${REPO_NAME} || aws ecr create-repository --repository-name ${REPO_NAME}\n",
    "\n",
    "docker push ${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/${REPO_NAME}:latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash build_and_push.sh $account_id $region $ecr_repository_name Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print('ECR training container ARN: {}'.format(train_image_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The docker image is now pushed to ECR. In the next section, we will show how to train an acoustic classification model using the custom container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Training on custom container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Training Data\n",
    "\n",
    "Upload data to S3, local training dataset has to be in Amazon S3 and the S3 URL to our dataset is passed into the fit() call. Due to the large dataset size, it will take a while for download to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"../data/UrbanSound8K\"\n",
    "\n",
    "train_data = sess.upload_data(\n",
    "    data,\n",
    "    bucket=bucket,\n",
    "    key_prefix=\"UrbanSound8K\",\n",
    ")\n",
    "\n",
    "train_data = sagemaker.session.s3_input(train_data,\n",
    "                                    distribution='FullyReplicated',\n",
    "                                    content_type='csv',\n",
    "                                    s3_data_type='S3Prefix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "\n",
    "Define the configuration of training to run. `container_image_uri` is where you can provide link to your custom container. Hyperparameters are fed into the training script with data directory (directory where the training dataset is stored).\n",
    "\n",
    "Epochs and cv have been set to low for training to complete fast. You can get 50%+ accuracy by setting epochs to 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyperparams = {'model': 'M5', # This is default model. You can implement addtional model in train.py\n",
    "               'epochs': 2, # Set to 2 for demo purpose\n",
    "               'batch-size': 128,\n",
    "               'cv': 0, # Set to 1 to perform 10 fold cross validation for all dataset\n",
    "               'stepsize': 20, # Optimizer stepsize\n",
    "               'num-workers': 30,\n",
    "              }\n",
    "\n",
    "pytorch_estimator = PyTorch(image_name=train_image_uri,\n",
    "                            entry_point='train.py',\n",
    "                            source_dir='../src',\n",
    "                            role=role,\n",
    "                            train_instance_type='ml.p3.8xlarge',\n",
    "                            train_instance_count=1,\n",
    "                            py_version='py3',\n",
    "                            framework_version='1.5.1',\n",
    "                            hyperparameters = hyperparams,\n",
    "                           )\n",
    "\n",
    "\n",
    "pytorch_estimator.fit({'training': train_data}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve model location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = pytorch_estimator.model_data\n",
    "print(model_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inference\n",
    "\n",
    "For inference, we will use default inference image. Mandatory `model_fn` is implemented in `inference.py`. PyTorchModel is used to deploy custom model that we trained previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_location, \n",
    "                             role=role, \n",
    "                             entry_point='inference.py',\n",
    "                             source_dir='../src',\n",
    "                             py_version='py3',\n",
    "                             framework_version='1.5.1',\n",
    "                            )\n",
    "predictor = pytorch_model.deploy(initial_instance_count=1, instance_type='ml.p2.8xlarge', wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install python package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install python packages to load sample test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q librosa==0.7.2 numba==0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference on sample test data\n",
    "\n",
    "Create dataloader to perform inference by batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, csv_path: Path, file_path: Path, folderList: Iterable[int], new_sr=8000, audio_len=20, sampling_ratio=5\n",
    "    ):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            csv_path (Path): Path to dataset metadata csv\n",
    "            file_path (Path): Path to data folders\n",
    "            folderList (Iterable[int]): Data folders to be included in dataset\n",
    "            new_sr (int, optional): New sampling rate. Defaults to 8000.\n",
    "            audio_len (int, optional): Audio length based on new sampling rate (sec). Defaults to 20.\n",
    "            sampling_ratio (int, optional): Additional downsampling ratio. Defaults to 5.\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.file_names = []\n",
    "        self.labels = []\n",
    "        self.folders = []\n",
    "        for i in range(0, len(df)):\n",
    "            if df.iloc[i, 5] in list(folderList):\n",
    "                self.labels.append(df.iloc[i, 6])\n",
    "                self.folders.append(df.iloc[i, 5])\n",
    "                temp = \"fold\" + str(df.iloc[i, 5]) + \"/\" + str(df.iloc[i, 0])\n",
    "                temp = file_path / temp\n",
    "                self.file_names.append(temp)\n",
    "\n",
    "        self.file_path = Path(file_path)\n",
    "        self.folderList = folderList\n",
    "        self.new_sr = new_sr\n",
    "        self.audio_len = audio_len\n",
    "        self.sampling_ratio = sampling_ratio\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # format the file path and load the file\n",
    "        path = self.file_names[index]\n",
    "        sound, sr = librosa.core.load(str(path), mono=False, sr=None)\n",
    "        if sound.ndim < 2:\n",
    "            sound = np.expand_dims(sound, axis=0)\n",
    "        # Convert into single channel format\n",
    "        sound = sound.mean(axis=0, keepdims=True)\n",
    "        # Downsampling\n",
    "        sound = librosa.core.resample(sound, orig_sr=sr, target_sr=self.new_sr)\n",
    "\n",
    "        # Zero padding to keep desired audio length in seconds\n",
    "        const_len = self.new_sr * self.audio_len\n",
    "        tempData = np.zeros([1, const_len])\n",
    "        if sound.shape[1] < const_len:\n",
    "            tempData[0, : sound.shape[1]] = sound[:]\n",
    "        else:\n",
    "            tempData[0, :] = sound[0, :const_len]\n",
    "        sound = tempData\n",
    "        # Resampling\n",
    "        new_const_len = const_len // self.sampling_ratio\n",
    "        soundFormatted = torch.zeros([1, new_const_len])\n",
    "        soundFormatted[0, :] = torch.tensor(sound[0, ::5], dtype=float)\n",
    "\n",
    "        return soundFormatted, self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = [10]\n",
    "datapath = Path(\"../data/UrbanSound8K\")\n",
    "csvpath = datapath / \"UrbanSound8K.csv\"\n",
    "\n",
    "test_set = UrbanSoundDataset(csvpath, datapath, test_folder)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(test_loader))\n",
    "print(X.shape, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the prediction returned from model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.predict(X.numpy())\n",
    "response = np.transpose(response, (1, 0, 2))\n",
    "prediction = response[0].argmax(axis=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Optional Cleanup\n",
    "\n",
    "When you're done with the endpoint, you should clean it up.\n",
    "\n",
    "All of the training jobs, models and endpoints we created can be viewed through the SageMaker console of your AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
